{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea9b5f3-f212-4321-a436-441da5955921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/aih/michal.kmicikiewicz/evodiff/evodiff\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pretrained import OA_DM_38M\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from Bio import SeqUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4def9e-f97a-406e-b772-167ac47456c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, collater, tokenizer, scheme = OA_DM_38M()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6aa60-e453-44c4-a450-c1918d4816d7",
   "metadata": {},
   "source": [
    "### generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "23aaf67a-3b0f-4295-bab7-72dab606dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mw_predictor(seq_list):\n",
    "    seq_len = len(seq_list[0])\n",
    "    return np.array([SeqUtils.molecular_weight(i, seq_type=\"protein\") for i in seq_list]) / seq_len\n",
    "\n",
    "\n",
    "class Sampler:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "\n",
    "    def generate(self, guide, seq_len, batch_size, resampling_steps, unroll_hop_size):    \n",
    "        sample = torch.zeros((batch_size, seq_len)) + self.tokenizer.mask_id\n",
    "        sample = sample.to(torch.long)\n",
    "        sample = sample.to(self.device)\n",
    "    \n",
    "        loc = np.arange(seq_len)\n",
    "        np.random.shuffle(loc)\n",
    "        with torch.no_grad():\n",
    "            for step, i in tqdm(enumerate(loc, start=-seq_len), total=len(loc)):\n",
    "                timestep = torch.tensor([0] * batch_size)\n",
    "                timestep = timestep.to(self.device)\n",
    "                prediction = self.model(sample, timestep)\n",
    "                p = prediction[:, i, :len(self.tokenizer.all_aas)-6]\n",
    "                p = torch.nn.functional.softmax(p, dim=1)\n",
    "                sampled_aa = torch.multinomial(p, num_samples=1).squeeze()\n",
    "                sample[:, i] = sampled_aa\n",
    "                if self.is_resampling_step(abs(step), resampling_steps):\n",
    "                    unrolled_sample = self.unroll(sample, loc[step+seq_len+1:], unroll_hop_size)\n",
    "                    preds = guide(unrolled_sample)\n",
    "                    ids = self.sample_exp_indices(torch.tensor(preds))\n",
    "                    sampled_aa = sampled_aa[ids]\n",
    "                    sample[:, i] = sampled_aa\n",
    "        untokenized = [self.tokenizer.untokenize(s) for s in sample]\n",
    "        return untokenized, loc\n",
    "\n",
    "    def unroll(self, sample, remaining_loc, hop_size):\n",
    "        for hop in range(0, len(remaining_loc), hop_size):\n",
    "            ids_chunk = remaining_loc[hop:hop+hop_size]\n",
    "            timestep = torch.tensor([0] * batch_size)\n",
    "            timestep = timestep.to(self.device)\n",
    "            prediction = self.model(sample, timestep)\n",
    "            p = prediction[:, ids_chunk, :len(self.tokenizer.all_aas)-6]\n",
    "            p = torch.nn.functional.softmax(p, dim=2)\n",
    "            p_flat = p.view(-1, p.shape[-1])\n",
    "            p_sample = torch.multinomial(p_flat, num_samples=1).squeeze()\n",
    "            sample[:, ids_chunk] = p_sample.view(p.shape[0], p.shape[1])\n",
    "        untokenized = [self.tokenizer.untokenize(s) for s in sample]\n",
    "        return untokenized\n",
    "        \n",
    "\n",
    "    def is_resampling_step(self, step, resampling_steps):\n",
    "        if isinstance(resampling_steps, list):\n",
    "            return step in resampling_steps\n",
    "        else:\n",
    "            return step > 1 and not step % resampling_steps\n",
    "\n",
    "    def sample_exp_indices(self, raw_scores, tau=1):\n",
    "        raw_scores = raw_scores - raw_scores.max()\n",
    "        weights = torch.exp(raw_scores / tau)\n",
    "        weights = weights / weights.sum()\n",
    "        return torch.multinomial(weights, len(weights), replacement=True)\n",
    "\n",
    "    def sample_lin_indices(self, raw_scores, tau=1):\n",
    "        raw_scores = raw_scores - raw_scores.min() \n",
    "        weights = raw_scores / raw_scores.sum()    \n",
    "        return torch.multinomial(weights, len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c73e9d25-d97e-427f-9e02-6545ee247193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_len = 20\n",
    "unroll_hop_size = 1\n",
    "resampling_steps = 1\n",
    "guide = mw_predictor\n",
    "\n",
    "\n",
    "sampler = Sampler(model, tokenizer)\n",
    "sequences, loc = sampler.generate(guide, seq_len, batch_size, resampling_steps, unroll_hop_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
